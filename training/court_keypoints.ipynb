{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7ba92b56",
      "metadata": {
        "id": "7ba92b56"
      },
      "source": [
        "# Dataload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "62eaf6d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62eaf6d9",
        "outputId": "856da737-c7dc-448c-c47d-c002e6825b97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.2.7-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from roboflow) (2025.8.3)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.12/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.4.9)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.0.2)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from roboflow) (11.3.0)\n",
            "Collecting pi-heif<2 (from roboflow)\n",
            "  Downloading pi_heif-1.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.5 kB)\n",
            "Collecting pillow-avif-plugin<2 (from roboflow)\n",
            "  Downloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.32.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.5.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (1.3.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (4.59.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (3.2.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->roboflow) (3.4.3)\n",
            "Downloading roboflow-1.2.7-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.6/88.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pi_heif-1.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m117.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: pillow-avif-plugin, filetype, pi-heif, opencv-python-headless, idna, roboflow\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.12.0.88\n",
            "    Uninstalling opencv-python-headless-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-headless-4.12.0.88\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pi-heif-1.1.0 pillow-avif-plugin-1.5.2 roboflow-1.2.7\n",
            "Collecting pytorch\n",
            "  Downloading pytorch-1.0.2.tar.gz (689 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pytorch\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for pytorch (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pytorch\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for pytorch\n",
            "Failed to build pytorch\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (pytorch)\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow\n",
        "!pip install pytorch\n",
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57dc2b9d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57dc2b9d",
        "outputId": "5917f1db-11fa-4b61-ca18-2e316ee85eed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading Dataset Version Zip in Court-Detection-7 to coco:: 100%|██████████| 42427/42427 [00:01<00:00, 33177.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Court-Detection-7 in coco:: 100%|██████████| 761/761 [00:00<00:00, 2433.23it/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from roboflow import Roboflow\n",
        "import shutil\n",
        "\n",
        "\n",
        "rf = Roboflow(api_key=\"APIKEY_HERE\")\n",
        "project = rf.workspace(\"ball-tracking-vcxpr\").project(\"court-detection-bxo2j-zts3d\")\n",
        "version = project.version(7)\n",
        "dataset = version.download(\"coco\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f4154b0",
      "metadata": {
        "id": "4f4154b0"
      },
      "source": [
        "# Torch Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "33fce9c9",
      "metadata": {
        "id": "33fce9c9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms, models\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "960117dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "960117dc",
        "outputId": "f14d7164-bde7-44bc-efd3-88dfe3ad9e18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4154c464",
      "metadata": {
        "id": "4154c464"
      },
      "source": [
        "# Torch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "90d74c80",
      "metadata": {
        "id": "90d74c80"
      },
      "outputs": [],
      "source": [
        "class KeyPointsDataset(Dataset):\n",
        "    def __init__(self, data_dir, data_file):\n",
        "        self.data_dir = data_dir\n",
        "        with open(data_file, 'r') as f:\n",
        "            self.data = json.load(f)\n",
        "\n",
        "        self.transforms = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data[\"images\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.data[\"images\"][idx][\"file_name\"]\n",
        "        img = cv2.imread(os.path.join(self.data_dir, img_path))\n",
        "        h, w = img.shape[:2]\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = self.transforms(img)\n",
        "\n",
        "        kps = np.array(self.data[\"annotations\"][idx][\"keypoints\"])\n",
        "        kps = kps.astype(np.float32)\n",
        "        kps[0::3] *= (224 / w)\n",
        "        kps[1::3] *= (224 / h)\n",
        "\n",
        "        kps = torch.tensor(kps, dtype=torch.float32)\n",
        "\n",
        "        return img, kps\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9643a9c5",
      "metadata": {
        "id": "9643a9c5"
      },
      "outputs": [],
      "source": [
        "train_data_dir = \"Court-Detection-7/train\"\n",
        "val_data_dir = \"Court-Detection-7/valid\"\n",
        "file_path = \"_annotations.coco.json\"\n",
        "train_dataset = KeyPointsDataset(train_data_dir, os.path.join(train_data_dir, file_path))\n",
        "val_dataset = KeyPointsDataset(val_data_dir, os.path.join(val_data_dir, file_path))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1b2f938",
      "metadata": {
        "id": "a1b2f938"
      },
      "source": [
        "# Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "993475c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "993475c9",
        "outputId": "9a055c90-a49c-4550-bbe4-52e84ee66d5a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 177MB/s]\n"
          ]
        }
      ],
      "source": [
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "keypoint_counds = 12\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, keypoint_counds * 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "40e99681",
      "metadata": {
        "id": "40e99681"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a3b02df",
      "metadata": {
        "id": "8a3b02df"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f6c699c7",
      "metadata": {
        "id": "f6c699c7"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5c93d25e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c93d25e",
        "outputId": "6614f59e-81bd-4b63-e41e-dd0d187dd92b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/125], Step [10/34], Loss: 8519.5195\n",
            "Epoch [1/125], Step [20/34], Loss: 6261.2485\n",
            "Epoch [1/125], Step [30/34], Loss: 4019.6311\n",
            "Epoch [2/125], Step [10/34], Loss: 2248.8945\n",
            "Epoch [2/125], Step [20/34], Loss: 1425.1730\n",
            "Epoch [2/125], Step [30/34], Loss: 916.3698\n",
            "Epoch [3/125], Step [10/34], Loss: 877.1524\n",
            "Epoch [3/125], Step [20/34], Loss: 418.3758\n",
            "Epoch [3/125], Step [30/34], Loss: 500.0815\n",
            "Epoch [4/125], Step [10/34], Loss: 376.3976\n",
            "Epoch [4/125], Step [20/34], Loss: 259.8979\n",
            "Epoch [4/125], Step [30/34], Loss: 135.2102\n",
            "Epoch [5/125], Step [10/34], Loss: 75.2014\n",
            "Epoch [5/125], Step [20/34], Loss: 276.9234\n",
            "Epoch [5/125], Step [30/34], Loss: 269.8593\n",
            "Epoch [6/125], Step [10/34], Loss: 268.2897\n",
            "Epoch [6/125], Step [20/34], Loss: 269.5298\n",
            "Epoch [6/125], Step [30/34], Loss: 340.0228\n",
            "Epoch [7/125], Step [10/34], Loss: 61.1354\n",
            "Epoch [7/125], Step [20/34], Loss: 63.9493\n",
            "Epoch [7/125], Step [30/34], Loss: 195.7274\n",
            "Epoch [8/125], Step [10/34], Loss: 228.3146\n",
            "Epoch [8/125], Step [20/34], Loss: 45.3601\n",
            "Epoch [8/125], Step [30/34], Loss: 239.4014\n",
            "Epoch [9/125], Step [10/34], Loss: 34.5926\n",
            "Epoch [9/125], Step [20/34], Loss: 149.1662\n",
            "Epoch [9/125], Step [30/34], Loss: 74.7774\n",
            "Epoch [10/125], Step [10/34], Loss: 121.8963\n",
            "Epoch [10/125], Step [20/34], Loss: 81.5906\n",
            "Epoch [10/125], Step [30/34], Loss: 233.4786\n",
            "Epoch [11/125], Step [10/34], Loss: 428.7130\n",
            "Epoch [11/125], Step [20/34], Loss: 123.4400\n",
            "Epoch [11/125], Step [30/34], Loss: 196.4937\n",
            "Epoch [12/125], Step [10/34], Loss: 33.8990\n",
            "Epoch [12/125], Step [20/34], Loss: 391.8039\n",
            "Epoch [12/125], Step [30/34], Loss: 314.1053\n",
            "Epoch [13/125], Step [10/34], Loss: 159.7473\n",
            "Epoch [13/125], Step [20/34], Loss: 43.2197\n",
            "Epoch [13/125], Step [30/34], Loss: 247.1215\n",
            "Epoch [14/125], Step [10/34], Loss: 59.1930\n",
            "Epoch [14/125], Step [20/34], Loss: 183.0619\n",
            "Epoch [14/125], Step [30/34], Loss: 585.8670\n",
            "Epoch [15/125], Step [10/34], Loss: 448.7593\n",
            "Epoch [15/125], Step [20/34], Loss: 167.3995\n",
            "Epoch [15/125], Step [30/34], Loss: 337.5144\n",
            "Epoch [16/125], Step [10/34], Loss: 161.0593\n",
            "Epoch [16/125], Step [20/34], Loss: 74.7110\n",
            "Epoch [16/125], Step [30/34], Loss: 103.4095\n",
            "Epoch [17/125], Step [10/34], Loss: 29.6159\n",
            "Epoch [17/125], Step [20/34], Loss: 149.2733\n",
            "Epoch [17/125], Step [30/34], Loss: 73.6377\n",
            "Epoch [18/125], Step [10/34], Loss: 471.6248\n",
            "Epoch [18/125], Step [20/34], Loss: 78.1835\n",
            "Epoch [18/125], Step [30/34], Loss: 26.3108\n",
            "Epoch [19/125], Step [10/34], Loss: 58.8939\n",
            "Epoch [19/125], Step [20/34], Loss: 40.5432\n",
            "Epoch [19/125], Step [30/34], Loss: 242.5440\n",
            "Epoch [20/125], Step [10/34], Loss: 164.2108\n",
            "Epoch [20/125], Step [20/34], Loss: 88.3222\n",
            "Epoch [20/125], Step [30/34], Loss: 101.4774\n",
            "Epoch [21/125], Step [10/34], Loss: 89.8716\n",
            "Epoch [21/125], Step [20/34], Loss: 136.8323\n",
            "Epoch [21/125], Step [30/34], Loss: 32.6182\n",
            "Epoch [22/125], Step [10/34], Loss: 175.4252\n",
            "Epoch [22/125], Step [20/34], Loss: 53.0220\n",
            "Epoch [22/125], Step [30/34], Loss: 48.7381\n",
            "Epoch [23/125], Step [10/34], Loss: 126.7194\n",
            "Epoch [23/125], Step [20/34], Loss: 171.8122\n",
            "Epoch [23/125], Step [30/34], Loss: 63.4766\n",
            "Epoch [24/125], Step [10/34], Loss: 92.9291\n",
            "Epoch [24/125], Step [20/34], Loss: 233.8811\n",
            "Epoch [24/125], Step [30/34], Loss: 355.3230\n",
            "Epoch [25/125], Step [10/34], Loss: 85.7441\n",
            "Epoch [25/125], Step [20/34], Loss: 47.3717\n",
            "Epoch [25/125], Step [30/34], Loss: 67.9327\n",
            "Epoch [26/125], Step [10/34], Loss: 148.8240\n",
            "Epoch [26/125], Step [20/34], Loss: 314.1724\n",
            "Epoch [26/125], Step [30/34], Loss: 90.4906\n",
            "Epoch [27/125], Step [10/34], Loss: 53.5201\n",
            "Epoch [27/125], Step [20/34], Loss: 144.0497\n",
            "Epoch [27/125], Step [30/34], Loss: 48.7533\n",
            "Epoch [28/125], Step [10/34], Loss: 25.9375\n",
            "Epoch [28/125], Step [20/34], Loss: 41.5259\n",
            "Epoch [28/125], Step [30/34], Loss: 46.3819\n",
            "Epoch [29/125], Step [10/34], Loss: 24.0727\n",
            "Epoch [29/125], Step [20/34], Loss: 62.8935\n",
            "Epoch [29/125], Step [30/34], Loss: 110.3895\n",
            "Epoch [30/125], Step [10/34], Loss: 14.9814\n",
            "Epoch [30/125], Step [20/34], Loss: 41.0516\n",
            "Epoch [30/125], Step [30/34], Loss: 18.8822\n",
            "Epoch [31/125], Step [10/34], Loss: 215.2611\n",
            "Epoch [31/125], Step [20/34], Loss: 129.3962\n",
            "Epoch [31/125], Step [30/34], Loss: 144.7899\n",
            "Epoch [32/125], Step [10/34], Loss: 186.9888\n",
            "Epoch [32/125], Step [20/34], Loss: 66.9785\n",
            "Epoch [32/125], Step [30/34], Loss: 26.4011\n",
            "Epoch [33/125], Step [10/34], Loss: 17.0223\n",
            "Epoch [33/125], Step [20/34], Loss: 101.0461\n",
            "Epoch [33/125], Step [30/34], Loss: 24.8071\n",
            "Epoch [34/125], Step [10/34], Loss: 14.3137\n",
            "Epoch [34/125], Step [20/34], Loss: 18.2146\n",
            "Epoch [34/125], Step [30/34], Loss: 25.7318\n",
            "Epoch [35/125], Step [10/34], Loss: 41.2481\n",
            "Epoch [35/125], Step [20/34], Loss: 10.9974\n",
            "Epoch [35/125], Step [30/34], Loss: 67.6548\n",
            "Epoch [36/125], Step [10/34], Loss: 11.4600\n",
            "Epoch [36/125], Step [20/34], Loss: 56.1764\n",
            "Epoch [36/125], Step [30/34], Loss: 18.6816\n",
            "Epoch [37/125], Step [10/34], Loss: 14.3675\n",
            "Epoch [37/125], Step [20/34], Loss: 37.7766\n",
            "Epoch [37/125], Step [30/34], Loss: 63.7479\n",
            "Epoch [38/125], Step [10/34], Loss: 11.0054\n",
            "Epoch [38/125], Step [20/34], Loss: 26.7901\n",
            "Epoch [38/125], Step [30/34], Loss: 65.8754\n",
            "Epoch [39/125], Step [10/34], Loss: 18.9849\n",
            "Epoch [39/125], Step [20/34], Loss: 8.5904\n",
            "Epoch [39/125], Step [30/34], Loss: 17.8045\n",
            "Epoch [40/125], Step [10/34], Loss: 31.2070\n",
            "Epoch [40/125], Step [20/34], Loss: 12.8412\n",
            "Epoch [40/125], Step [30/34], Loss: 35.2580\n",
            "Epoch [41/125], Step [10/34], Loss: 14.4020\n",
            "Epoch [41/125], Step [20/34], Loss: 8.3946\n",
            "Epoch [41/125], Step [30/34], Loss: 185.4042\n",
            "Epoch [42/125], Step [10/34], Loss: 41.1369\n",
            "Epoch [42/125], Step [20/34], Loss: 65.6178\n",
            "Epoch [42/125], Step [30/34], Loss: 30.8870\n",
            "Epoch [43/125], Step [10/34], Loss: 8.8713\n",
            "Epoch [43/125], Step [20/34], Loss: 8.1525\n",
            "Epoch [43/125], Step [30/34], Loss: 7.7027\n",
            "Epoch [44/125], Step [10/34], Loss: 4.2753\n",
            "Epoch [44/125], Step [20/34], Loss: 175.3772\n",
            "Epoch [44/125], Step [30/34], Loss: 45.3576\n",
            "Epoch [45/125], Step [10/34], Loss: 9.4304\n",
            "Epoch [45/125], Step [20/34], Loss: 9.3522\n",
            "Epoch [45/125], Step [30/34], Loss: 12.6243\n",
            "Epoch [46/125], Step [10/34], Loss: 34.2560\n",
            "Epoch [46/125], Step [20/34], Loss: 15.9096\n",
            "Epoch [46/125], Step [30/34], Loss: 12.5238\n",
            "Epoch [47/125], Step [10/34], Loss: 70.4064\n",
            "Epoch [47/125], Step [20/34], Loss: 77.2334\n",
            "Epoch [47/125], Step [30/34], Loss: 7.8997\n",
            "Epoch [48/125], Step [10/34], Loss: 19.2803\n",
            "Epoch [48/125], Step [20/34], Loss: 54.1240\n",
            "Epoch [48/125], Step [30/34], Loss: 5.4225\n",
            "Epoch [49/125], Step [10/34], Loss: 11.9060\n",
            "Epoch [49/125], Step [20/34], Loss: 9.7875\n",
            "Epoch [49/125], Step [30/34], Loss: 275.4934\n",
            "Epoch [50/125], Step [10/34], Loss: 53.1648\n",
            "Epoch [50/125], Step [20/34], Loss: 17.5776\n",
            "Epoch [50/125], Step [30/34], Loss: 49.3477\n",
            "Epoch [51/125], Step [10/34], Loss: 13.9486\n",
            "Epoch [51/125], Step [20/34], Loss: 8.5328\n",
            "Epoch [51/125], Step [30/34], Loss: 51.4883\n",
            "Epoch [52/125], Step [10/34], Loss: 25.2783\n",
            "Epoch [52/125], Step [20/34], Loss: 16.9920\n",
            "Epoch [52/125], Step [30/34], Loss: 38.9036\n",
            "Epoch [53/125], Step [10/34], Loss: 10.7747\n",
            "Epoch [53/125], Step [20/34], Loss: 7.9544\n",
            "Epoch [53/125], Step [30/34], Loss: 7.3600\n",
            "Epoch [54/125], Step [10/34], Loss: 6.7644\n",
            "Epoch [54/125], Step [20/34], Loss: 42.2278\n",
            "Epoch [54/125], Step [30/34], Loss: 30.8402\n",
            "Epoch [55/125], Step [10/34], Loss: 3.6891\n",
            "Epoch [55/125], Step [20/34], Loss: 7.3050\n",
            "Epoch [55/125], Step [30/34], Loss: 8.0298\n",
            "Epoch [56/125], Step [10/34], Loss: 27.1025\n",
            "Epoch [56/125], Step [20/34], Loss: 9.5649\n",
            "Epoch [56/125], Step [30/34], Loss: 4.3406\n",
            "Epoch [57/125], Step [10/34], Loss: 29.6764\n",
            "Epoch [57/125], Step [20/34], Loss: 14.7387\n",
            "Epoch [57/125], Step [30/34], Loss: 7.2708\n",
            "Epoch [58/125], Step [10/34], Loss: 27.0989\n",
            "Epoch [58/125], Step [20/34], Loss: 5.5881\n",
            "Epoch [58/125], Step [30/34], Loss: 5.8879\n",
            "Epoch [59/125], Step [10/34], Loss: 7.3185\n",
            "Epoch [59/125], Step [20/34], Loss: 9.6470\n",
            "Epoch [59/125], Step [30/34], Loss: 13.5859\n",
            "Epoch [60/125], Step [10/34], Loss: 4.5273\n",
            "Epoch [60/125], Step [20/34], Loss: 3.6032\n",
            "Epoch [60/125], Step [30/34], Loss: 4.9694\n",
            "Epoch [61/125], Step [10/34], Loss: 7.5860\n",
            "Epoch [61/125], Step [20/34], Loss: 2.9828\n",
            "Epoch [61/125], Step [30/34], Loss: 18.7325\n",
            "Epoch [62/125], Step [10/34], Loss: 3.8664\n",
            "Epoch [62/125], Step [20/34], Loss: 1.7245\n",
            "Epoch [62/125], Step [30/34], Loss: 5.5433\n",
            "Epoch [63/125], Step [10/34], Loss: 3.8813\n",
            "Epoch [63/125], Step [20/34], Loss: 2.7863\n",
            "Epoch [63/125], Step [30/34], Loss: 2.9004\n",
            "Epoch [64/125], Step [10/34], Loss: 30.8817\n",
            "Epoch [64/125], Step [20/34], Loss: 3.1588\n",
            "Epoch [64/125], Step [30/34], Loss: 2.2294\n",
            "Epoch [65/125], Step [10/34], Loss: 4.8901\n",
            "Epoch [65/125], Step [20/34], Loss: 1.5763\n",
            "Epoch [65/125], Step [30/34], Loss: 4.3273\n",
            "Epoch [66/125], Step [10/34], Loss: 3.9368\n",
            "Epoch [66/125], Step [20/34], Loss: 4.7302\n",
            "Epoch [66/125], Step [30/34], Loss: 5.6524\n",
            "Epoch [67/125], Step [10/34], Loss: 7.9728\n",
            "Epoch [67/125], Step [20/34], Loss: 13.8357\n",
            "Epoch [67/125], Step [30/34], Loss: 15.8572\n",
            "Epoch [68/125], Step [10/34], Loss: 9.1473\n",
            "Epoch [68/125], Step [20/34], Loss: 4.6640\n",
            "Epoch [68/125], Step [30/34], Loss: 5.9512\n",
            "Epoch [69/125], Step [10/34], Loss: 6.1597\n",
            "Epoch [69/125], Step [20/34], Loss: 2.6012\n",
            "Epoch [69/125], Step [30/34], Loss: 4.3188\n",
            "Epoch [70/125], Step [10/34], Loss: 12.9189\n",
            "Epoch [70/125], Step [20/34], Loss: 4.7624\n",
            "Epoch [70/125], Step [30/34], Loss: 3.4789\n",
            "Epoch [71/125], Step [10/34], Loss: 2.6829\n",
            "Epoch [71/125], Step [20/34], Loss: 3.0520\n",
            "Epoch [71/125], Step [30/34], Loss: 5.9694\n",
            "Epoch [72/125], Step [10/34], Loss: 18.5431\n",
            "Epoch [72/125], Step [20/34], Loss: 3.1553\n",
            "Epoch [72/125], Step [30/34], Loss: 2.8777\n",
            "Epoch [73/125], Step [10/34], Loss: 4.3133\n",
            "Epoch [73/125], Step [20/34], Loss: 4.2145\n",
            "Epoch [73/125], Step [30/34], Loss: 4.2121\n",
            "Epoch [74/125], Step [10/34], Loss: 9.3793\n",
            "Epoch [74/125], Step [20/34], Loss: 5.3471\n",
            "Epoch [74/125], Step [30/34], Loss: 7.5544\n",
            "Epoch [75/125], Step [10/34], Loss: 3.9739\n",
            "Epoch [75/125], Step [20/34], Loss: 3.5870\n",
            "Epoch [75/125], Step [30/34], Loss: 1.1773\n",
            "Epoch [76/125], Step [10/34], Loss: 2.7291\n",
            "Epoch [76/125], Step [20/34], Loss: 1.9366\n",
            "Epoch [76/125], Step [30/34], Loss: 2.4027\n",
            "Epoch [77/125], Step [10/34], Loss: 2.4524\n",
            "Epoch [77/125], Step [20/34], Loss: 2.4668\n",
            "Epoch [77/125], Step [30/34], Loss: 3.0915\n",
            "Epoch [78/125], Step [10/34], Loss: 1.3992\n",
            "Epoch [78/125], Step [20/34], Loss: 3.4042\n",
            "Epoch [78/125], Step [30/34], Loss: 3.6717\n",
            "Epoch [79/125], Step [10/34], Loss: 1.6798\n",
            "Epoch [79/125], Step [20/34], Loss: 2.3662\n",
            "Epoch [79/125], Step [30/34], Loss: 4.1301\n",
            "Epoch [80/125], Step [10/34], Loss: 4.3056\n",
            "Epoch [80/125], Step [20/34], Loss: 5.3075\n",
            "Epoch [80/125], Step [30/34], Loss: 2.2720\n",
            "Epoch [81/125], Step [10/34], Loss: 5.1538\n",
            "Epoch [81/125], Step [20/34], Loss: 4.4677\n",
            "Epoch [81/125], Step [30/34], Loss: 1.1062\n",
            "Epoch [82/125], Step [10/34], Loss: 2.9341\n",
            "Epoch [82/125], Step [20/34], Loss: 1.5985\n",
            "Epoch [82/125], Step [30/34], Loss: 1.3104\n",
            "Epoch [83/125], Step [10/34], Loss: 1.7317\n",
            "Epoch [83/125], Step [20/34], Loss: 3.2612\n",
            "Epoch [83/125], Step [30/34], Loss: 4.1380\n",
            "Epoch [84/125], Step [10/34], Loss: 8.7355\n",
            "Epoch [84/125], Step [20/34], Loss: 10.2433\n",
            "Epoch [84/125], Step [30/34], Loss: 2.6298\n",
            "Epoch [85/125], Step [10/34], Loss: 4.3080\n",
            "Epoch [85/125], Step [20/34], Loss: 1.2555\n",
            "Epoch [85/125], Step [30/34], Loss: 1.4142\n",
            "Epoch [86/125], Step [10/34], Loss: 1.6228\n",
            "Epoch [86/125], Step [20/34], Loss: 4.3677\n",
            "Epoch [86/125], Step [30/34], Loss: 3.0950\n",
            "Epoch [87/125], Step [10/34], Loss: 1.4232\n",
            "Epoch [87/125], Step [20/34], Loss: 2.3580\n",
            "Epoch [87/125], Step [30/34], Loss: 1.0920\n",
            "Epoch [88/125], Step [10/34], Loss: 2.7995\n",
            "Epoch [88/125], Step [20/34], Loss: 6.3952\n",
            "Epoch [88/125], Step [30/34], Loss: 4.2796\n",
            "Epoch [89/125], Step [10/34], Loss: 3.6172\n",
            "Epoch [89/125], Step [20/34], Loss: 1.9731\n",
            "Epoch [89/125], Step [30/34], Loss: 2.0359\n",
            "Epoch [90/125], Step [10/34], Loss: 5.0643\n",
            "Epoch [90/125], Step [20/34], Loss: 2.1581\n",
            "Epoch [90/125], Step [30/34], Loss: 2.0269\n",
            "Epoch [91/125], Step [10/34], Loss: 7.3806\n",
            "Epoch [91/125], Step [20/34], Loss: 4.9227\n",
            "Epoch [91/125], Step [30/34], Loss: 2.6110\n",
            "Epoch [92/125], Step [10/34], Loss: 3.1069\n",
            "Epoch [92/125], Step [20/34], Loss: 0.8036\n",
            "Epoch [92/125], Step [30/34], Loss: 1.4070\n",
            "Epoch [93/125], Step [10/34], Loss: 1.4614\n",
            "Epoch [93/125], Step [20/34], Loss: 2.9627\n",
            "Epoch [93/125], Step [30/34], Loss: 2.2850\n",
            "Epoch [94/125], Step [10/34], Loss: 3.3812\n",
            "Epoch [94/125], Step [20/34], Loss: 0.9844\n",
            "Epoch [94/125], Step [30/34], Loss: 2.7766\n",
            "Epoch [95/125], Step [10/34], Loss: 1.2535\n",
            "Epoch [95/125], Step [20/34], Loss: 10.4762\n",
            "Epoch [95/125], Step [30/34], Loss: 2.3456\n",
            "Epoch [96/125], Step [10/34], Loss: 1.9722\n",
            "Epoch [96/125], Step [20/34], Loss: 2.5539\n",
            "Epoch [96/125], Step [30/34], Loss: 1.2773\n",
            "Epoch [97/125], Step [10/34], Loss: 1.0978\n",
            "Epoch [97/125], Step [20/34], Loss: 2.1624\n",
            "Epoch [97/125], Step [30/34], Loss: 2.2496\n",
            "Epoch [98/125], Step [10/34], Loss: 6.2424\n",
            "Epoch [98/125], Step [20/34], Loss: 1.9095\n",
            "Epoch [98/125], Step [30/34], Loss: 3.2205\n",
            "Epoch [99/125], Step [10/34], Loss: 11.5638\n",
            "Epoch [99/125], Step [20/34], Loss: 11.3678\n",
            "Epoch [99/125], Step [30/34], Loss: 2.0615\n",
            "Epoch [100/125], Step [10/34], Loss: 6.4519\n",
            "Epoch [100/125], Step [20/34], Loss: 18.2843\n",
            "Epoch [100/125], Step [30/34], Loss: 8.2771\n",
            "Epoch [101/125], Step [10/34], Loss: 10.1445\n",
            "Epoch [101/125], Step [20/34], Loss: 4.1541\n",
            "Epoch [101/125], Step [30/34], Loss: 1.7428\n",
            "Epoch [102/125], Step [10/34], Loss: 1.1498\n",
            "Epoch [102/125], Step [20/34], Loss: 2.5109\n",
            "Epoch [102/125], Step [30/34], Loss: 1.9459\n",
            "Epoch [103/125], Step [10/34], Loss: 1.7740\n",
            "Epoch [103/125], Step [20/34], Loss: 1.4768\n",
            "Epoch [103/125], Step [30/34], Loss: 1.7790\n",
            "Epoch [104/125], Step [10/34], Loss: 3.9094\n",
            "Epoch [104/125], Step [20/34], Loss: 2.2338\n",
            "Epoch [104/125], Step [30/34], Loss: 1.0465\n",
            "Epoch [105/125], Step [10/34], Loss: 2.2028\n",
            "Epoch [105/125], Step [20/34], Loss: 1.4264\n",
            "Epoch [105/125], Step [30/34], Loss: 4.3669\n",
            "Epoch [106/125], Step [10/34], Loss: 4.6209\n",
            "Epoch [106/125], Step [20/34], Loss: 1.4602\n",
            "Epoch [106/125], Step [30/34], Loss: 1.4956\n",
            "Epoch [107/125], Step [10/34], Loss: 4.7046\n",
            "Epoch [107/125], Step [20/34], Loss: 2.3345\n",
            "Epoch [107/125], Step [30/34], Loss: 1.7609\n",
            "Epoch [108/125], Step [10/34], Loss: 1.5823\n",
            "Epoch [108/125], Step [20/34], Loss: 1.9841\n",
            "Epoch [108/125], Step [30/34], Loss: 1.7260\n",
            "Epoch [109/125], Step [10/34], Loss: 1.3491\n",
            "Epoch [109/125], Step [20/34], Loss: 2.5984\n",
            "Epoch [109/125], Step [30/34], Loss: 1.5417\n",
            "Epoch [110/125], Step [10/34], Loss: 1.6851\n",
            "Epoch [110/125], Step [20/34], Loss: 2.0746\n",
            "Epoch [110/125], Step [30/34], Loss: 0.9399\n",
            "Epoch [111/125], Step [10/34], Loss: 2.0492\n",
            "Epoch [111/125], Step [20/34], Loss: 1.0354\n",
            "Epoch [111/125], Step [30/34], Loss: 1.3705\n",
            "Epoch [112/125], Step [10/34], Loss: 1.4317\n",
            "Epoch [112/125], Step [20/34], Loss: 2.5007\n",
            "Epoch [112/125], Step [30/34], Loss: 1.0900\n",
            "Epoch [113/125], Step [10/34], Loss: 3.0160\n",
            "Epoch [113/125], Step [20/34], Loss: 0.8993\n",
            "Epoch [113/125], Step [30/34], Loss: 0.9921\n",
            "Epoch [114/125], Step [10/34], Loss: 1.9453\n",
            "Epoch [114/125], Step [20/34], Loss: 1.2361\n",
            "Epoch [114/125], Step [30/34], Loss: 2.3124\n",
            "Epoch [115/125], Step [10/34], Loss: 3.6429\n",
            "Epoch [115/125], Step [20/34], Loss: 2.1333\n",
            "Epoch [115/125], Step [30/34], Loss: 1.7539\n",
            "Epoch [116/125], Step [10/34], Loss: 2.2168\n",
            "Epoch [116/125], Step [20/34], Loss: 1.4182\n",
            "Epoch [116/125], Step [30/34], Loss: 1.8076\n",
            "Epoch [117/125], Step [10/34], Loss: 3.6868\n",
            "Epoch [117/125], Step [20/34], Loss: 2.2850\n",
            "Epoch [117/125], Step [30/34], Loss: 1.2565\n",
            "Epoch [118/125], Step [10/34], Loss: 4.2981\n",
            "Epoch [118/125], Step [20/34], Loss: 0.6256\n",
            "Epoch [118/125], Step [30/34], Loss: 1.2970\n",
            "Epoch [119/125], Step [10/34], Loss: 1.1534\n",
            "Epoch [119/125], Step [20/34], Loss: 1.0545\n",
            "Epoch [119/125], Step [30/34], Loss: 2.2093\n",
            "Epoch [120/125], Step [10/34], Loss: 0.6208\n",
            "Epoch [120/125], Step [20/34], Loss: 1.2414\n",
            "Epoch [120/125], Step [30/34], Loss: 1.9498\n",
            "Epoch [121/125], Step [10/34], Loss: 2.5634\n",
            "Epoch [121/125], Step [20/34], Loss: 1.4493\n",
            "Epoch [121/125], Step [30/34], Loss: 3.0220\n",
            "Epoch [122/125], Step [10/34], Loss: 1.1073\n",
            "Epoch [122/125], Step [20/34], Loss: 2.6577\n",
            "Epoch [122/125], Step [30/34], Loss: 2.0452\n",
            "Epoch [123/125], Step [10/34], Loss: 1.0276\n",
            "Epoch [123/125], Step [20/34], Loss: 2.9240\n",
            "Epoch [123/125], Step [30/34], Loss: 1.4096\n",
            "Epoch [124/125], Step [10/34], Loss: 1.2134\n",
            "Epoch [124/125], Step [20/34], Loss: 0.6116\n",
            "Epoch [124/125], Step [30/34], Loss: 0.8992\n",
            "Epoch [125/125], Step [10/34], Loss: 1.7049\n",
            "Epoch [125/125], Step [20/34], Loss: 1.2142\n",
            "Epoch [125/125], Step [30/34], Loss: 4.6017\n"
          ]
        }
      ],
      "source": [
        "epoches = 125\n",
        "for epoch in range(epoches):\n",
        "    for i, (images, keypoints) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        keypoints = keypoints.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, keypoints)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epoches}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "97048feb",
      "metadata": {
        "id": "97048feb"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'keypoints_model_v4.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "SeJ7uzyeJ-0T",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeJ7uzyeJ-0T",
        "outputId": "e813ad2a-ff48-4ddc-d7d5-c56e05a4cda2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average MAE: 1.6087\n",
            "Average MSE: 61.9435\n",
            "Average RMSE: 7.8704\n"
          ]
        }
      ],
      "source": [
        "model.eval()  # Set the model to evaluation mode\n",
        "total_mae = 0.0\n",
        "total_mse = 0.0\n",
        "num_samples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in val_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Make predictions\n",
        "        predictions = model(inputs)\n",
        "\n",
        "        # Calculate batch metrics\n",
        "        batch_mae = torch.mean(torch.abs(predictions - targets))\n",
        "        batch_mse = torch.mean((predictions - targets) ** 2)\n",
        "\n",
        "        # Accumulate totals\n",
        "        total_mae += batch_mae.item() * inputs.size(0)\n",
        "        total_mse += batch_mse.item() * inputs.size(0)\n",
        "        num_samples += inputs.size(0)\n",
        "\n",
        "# Calculate overall metrics\n",
        "avg_mae = total_mae / num_samples\n",
        "avg_mse = total_mse / num_samples\n",
        "avg_rmse = torch.sqrt(torch.tensor(avg_mse)).item()\n",
        "\n",
        "print(f'Average MAE: {avg_mae:.4f}')\n",
        "print(f'Average MSE: {avg_mse:.4f}')\n",
        "print(f'Average RMSE: {avg_rmse:.4f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
